{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling\n",
    "\n",
    "The goal of this notebook is two-fold:\n",
    "* identify the main topics of the corpus through approach (unsupervised clustering)\n",
    "* build a text classifier based on these topics (supervised learning).\n",
    "\n",
    "### Contents\n",
    "\n",
    "__Preliminaries__\n",
    "* a. Overview of text modeling techniques\n",
    "* b. Imports\n",
    "\n",
    "__1. LDA__\n",
    "\n",
    "__2. Clustering__\n",
    "* a. Tf-Idf for Clustering\n",
    "* b. K-Means\n",
    "* c. DBSCAN\n",
    "\n",
    "__3. Topic classifier__\n",
    "\n",
    "What's next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__a. Overview of text modeling techniques__\n",
    "\n",
    "* [LDA](https://towardsdatascience.com/end-to-end-topic-modeling-in-python-latent-dirichlet-allocation-lda-35ce4ed6b3e0)\n",
    "\n",
    "* Clustering. In order to build clusters we have to transform our news articles into a numerical representation that models can handle. Here we go for old good tf-idf vectors (we will test more state-of-the-art techniques in the next steps). Algorithms: K-Means and DBSCAN seem like the most relevant options to begin with.\n",
    "\n",
    "* ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__b. Imports__\n",
    "\n",
    "If you don't have already done so, please download these NLTK dependancies by running the following lines:\n",
    "\n",
    "> nltk.download('wordnet')\n",
    "\n",
    "> nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fr_core_news_sm loaded\n"
     ]
    }
   ],
   "source": [
    "# Classic packages\n",
    "\n",
    "import os \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import random \n",
    "random.seed(a=2905) # set random seed \n",
    "import pickle\n",
    "\n",
    "\n",
    "# NLP packages\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "\n",
    "import spacy\n",
    "try: \n",
    "    print(\"fr_core_news_sm loaded\")\n",
    "    nlp = spacy.load(\"fr_core_news_sm\") # load pre-trained models for French\n",
    "except:\n",
    "    print(\"fr loaded\")\n",
    "    nlp=spacy.load('fr') # fr calls fr_core_news_sm \n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem.wordnet import WordNetLemmatizer # not adapted to French?\n",
    "from nltk.stem.snowball import FrenchStemmer # already something \n",
    "# --> Lemmatizer would be better --> use spaCy lemmatizer\n",
    "\n",
    "# ML with sklearn\n",
    "import sklearn.cluster\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer \n",
    "from sklearn.feature_extraction.text import  TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "news_df=pd.read_csv(\"./articles.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Latent Dirichlet Allocation (LDA)\n",
    "\n",
    "\n",
    "Main idea: Each document is represented as a distribution over topics, and each topic is represented as a distribution over words.\n",
    "Here we do not set the number of topics in advance, we rather set it arbitrarily like a threshold and see if the results are relevant.\n",
    "\n",
    "Code freely adapted from this [TDS post](https://towardsdatascience.com/topic-modelling-in-python-with-nltk-and-gensim-4ef03213cd21).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Text cleaning for LDA__\n",
    "\n",
    "* tokenize words (here using spacy parser for French)\n",
    "* lemmatize (using NLTK WordNetLemmatizer)\n",
    "* stopwords removal (using the default NLTK stopwords list for French)\n",
    "* apply pipeline on titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## spacy LDA\n",
    "\n",
    "spacy.load('fr')\n",
    "from spacy.lang.fr import French\n",
    "parser = French()\n",
    "\n",
    "def tokenize(text):\n",
    "    lda_tokens = []\n",
    "    tokens = parser(text)\n",
    "    for token in tokens:\n",
    "        if token.orth_.isspace():\n",
    "            continue\n",
    "        elif token.like_url:\n",
    "            lda_tokens.append('URL')\n",
    "        elif token.orth_.startswith('@'):\n",
    "            lda_tokens.append('SCREEN_NAME')\n",
    "        else:\n",
    "            lda_tokens.append(token.lower_)\n",
    "    return lda_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'une', 'es', 'aurions', 'fussions', 'serez', 'serait', 's', 'au', 'quarante', 'y', 'serais', 'fusses', 'étaient', 'serions', 'notre', 'dans', 'étantes', 'dix', 'il', 'sa', 'trois', 'suis', 'ayante', 'aies', 'ayantes', 'des', 'serai', 'ton', 'ne', 'te', 'aurez', 'auras', 'c', 'soixante', 'eûmes', 'ses', 'je', 'mon', 'cent', 'pas', 'soit', 'seriez', 'que', 'quatorze', 'ayant', 'fussiez', 'neuf', 'par', 'cinq', 'eût', 'ces', 'ayez', 'aviez', 'fussent', 'en', 'étante', 'auraient', 'avez', 'ait', 'ou', 'deux', 'avais', 'as', 'seront', 'même', 'qui', 'avec', 'un', 'étant', 'tu', 'leur', 'fûmes', 'aient', 'avaient', 'tes', 'étiez', 'lui', 'son', 'sur', 'le', 'est', 'eussions', 'vos', 'qu', 'étés', 'fût', 'se', 'aurai', 'cinquante', 'eurent', 'été', 'eussent', 'elle', 'treize', 'douze', 'quinze', 't', 'n', 'six', 'étée', 'mais', 'la', 'fut', 'l', 'seraient', 'mes', 'toi', 'aurons', 'sommes', 'les', 'eusse', 'êtes', 'étions', 'd', 'm', 'eus', 'sont', 'quatre', 'du', 'eues', 'seize', 'trente', 'eûtes', 'de', 'fus', 'sois', 'ma', 'eue', 'aurais', 'auront', 'on', 'sept', 'aurait', 'ils', 'ta', 'et', 'étants', 'était', 'pour', 'me', 'huit', 'vous', 'soyez', 'soient', 'avions', 'soyons', 'ai', 'vingt', 'fusse', 'j', 'eusses', 'eux', 'étées', 'nous', 'avons', 'eu', 'votre', 'sera', 'ce', 'onze', 'aie', 'furent', 'avait', 'étais', 'ayants', 'aux', 'aura', 'auriez', 'à', 'eut', 'moi', 'nos', 'ont', 'ayons', 'eussiez', 'serons', 'fûtes', 'seras'}\n"
     ]
    }
   ],
   "source": [
    "# stopwords removal\n",
    "fr_stop = set(nltk.corpus.stopwords.words('french'))\n",
    "my_fr_stop=fr_stop.union({'un' ,'deux','trois','quatre','cinq','six','sept','huit','neuf','dix','onze', 'douze', \n",
    "                          'treize','quatorze', 'quinze', 'seize', 'vingt', 'trente', \n",
    "                          'quarante', 'cinquante','soixante','cent'}, fr_stop)\n",
    "#'mille'  'million' 'milliard' 'billion' are not added to the stopwords list because they are more discriminative\n",
    "\n",
    "print(my_fr_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lemmas\n",
      "les --> le\n",
      "manifestations --> manifestation\n",
      "qui --> qui\n",
      "ont --> avoir\n",
      "agitées --> agiter\n",
      "la --> le\n",
      "France --> France\n",
      "ces --> ce\n",
      "derniers --> dernier\n",
      "mois --> mois\n",
      "\n",
      "Stems\n",
      "les --> le\n",
      "manifestations --> manifest\n",
      "qui --> qui\n",
      "ont --> ont\n",
      "agitées --> agit\n",
      "la --> la\n",
      "France --> franc\n",
      "ces --> ce\n",
      "derniers --> derni\n",
      "mois --> mois\n"
     ]
    }
   ],
   "source": [
    "# Comparison: nltk french stemmer vs spacy french lemmatizer\n",
    "\n",
    "# spacy\n",
    "print(\"\\nLemmas\")\n",
    "doc = nlp(u\"les manifestations qui ont agitées la France ces derniers mois\")\n",
    "for token in doc:\n",
    "    print(token, '-->', token.lemma_)\n",
    "print(\"\\nStems\")\n",
    "stemmer = FrenchStemmer()\n",
    "for w in \"les manifestations qui ont agitées la France ces derniers mois\".split():\n",
    "    print(w, '-->', stemmer.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize, delete token if too small, \n",
    "def prepare_text_for_lda(text):\n",
    "    tokens = tokenize(text)\n",
    "    tokens = [token for token in tokens if len(token) > 4] # arbitrary\n",
    "    tokens = [token for token in tokens if token not in fr_stop]\n",
    "    doc = nlp(' '.join(tokens))\n",
    "    tokens = [token.lemma_ for token in doc]\n",
    "    return tokens  #[t.encode('utf-8') for t in tokens] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['décembre', 'attentat', 'contre', 'carrero', 'blanco']\n",
      "['tavernier', 'noiret', 'union', 'sacrer']\n",
      "['michel', 'hermon']\n",
      "['droit']\n",
      "['henri', 'castrier', 'regner', 'royaume']\n",
      "['maltraitance']\n",
      "['france', 'activite', 'industriel', 'hausse', 'ralentir']\n",
      "['avril', 'réacteur', 'central', 'explose', 'après', 'voici', 'premier', 'histoire', 'oser', 'affronter', 'radiation', 'impression', 'entrer', 'tombeau', 'humanité', 'janvier', 'photographe', 'russe', 'victoria', 'ivleva', 'prépare', 'pénétrer', 'central', 'nucléaire', 'tchernobyl', 'maudire', 'voilà', 'presque', 'explosion', 'terrifiante', 'ravager', 'réacteur', 'disperser', 'tout', 'europe', 'poison', 'radioactif', 'rester', 'quart', 'heure', 'escalader', 'vestige', 'matérial', 'tordu', 'entasser', 'fondu', 'revêtir', 'premier', 'combinaison', 'coton', 'blanc', 'second', 'plastique', 'botte', 'gant', 'toqu', 'masqu', 'respirateur', 'victoria', 'ivleva', 'marche', 'interdire', 'homme', 'youri', 'kovsar', 'mikhailov', 'accompagnent', 'enfer', 'moderne', 'danger', 'silencieux', 'invisible', 'impalpable', 'inster', 'attendre', 'longtemps', 'longs', 'pendant', 'lequel', 'victoria', 'moscovite', 'rendre', 'régulièrement', 'tchernobyl', 'cherchant', 'inlassablement', 'moyen', 'introduir', 'sarcophage', 'béton', 'métal', 'recouvr', 'bâtiment', 'dévaster', 'scientifique', 'beaucoup', 'hésiter', 'avant', 'accepter', 'guider', 'visiteur', 'clandestine', \"aujourd'hui\", 'gravissent', 'ensembl', 'lourdement', 'escalier', 'conduire', 'avant', 'avril', 'salle', 'central', 'celui', 'affleurer', 'partie', 'supérieur', 'réacteur', 'situer', 'environ', 'mètre', 'premier', 'arrêt', 'niveau', 'premières', 'photo', 'radioactivité', 'ambiant', 'heure', 'titre', 'comparaison', 'autorité', 'international', 'fixent', 'exposition', 'maximal', 'population', 'celui', 'travailleurs', 'industrie', 'nucléaire', 'grimpent', 'échell', 'vertical', 'avant', 'courir', 'milieu', 'champ', 'ruine', 'baigner', 'radiation', 'minute', 'perdre', 'question', 'avoir', 'question', 'éloigner', 'chemin', 'tracer', 'stalker', 'passeurs', 'ainsi', 'surnomment', 'même', 'hommage', 'prémonitoire', 'andreï', 'tarkovski', 'silence', 'pénombre', 'percent', 'rayon', 'lumière', 'venir', 'trous', 'sarcophage', 'aperçoit', 'cratèr', 'avant', 'catastrophe', 'battre', 'nucléaire', 'réacteur', 'sensation', 'fugitif', 'brutal', 'horreur', 'demi-tour', 'lumière', 'neige', 'recouvr', 'ukraine', 'suite', 'abord', 'dévêtir', 'doucher', 'soigneusement', 'produit', 'désactivateur', 'enfermer', 'tenu', 'spécial', 'hermétique', 'lui', 'finir', 'carrière', 'parmi', 'autre', 'objet', 'contaminé', 'cimetièr', 'radioactif', 'tchernobyl', 'victoria', 'ivleva', 'ajoute', 'repérage', 'perdu', 'long', 'minute', 'ramper', 'tunnel', 'ressent', 'intérieur', 'monumer', 'atome', 'défunt', 'appartement', 'centre', 'moscou', 'victoria', 'ivleva', 'répondre', 'citer', 'pouchkine', 'existe', 'enchantement', 'combat', 'précipic', 'lugubre', 'côter', 'youri', 'kovsar', 'mikhailov', 'venir', 'spécialemer', 'ukrain', 'marier', 'enfant', 'histoire', 'jumeau', 'depuis', 'tchernobyl', 'croiser', 'chemin', 'dosimétriste', 'professionnel', 'nucléaire', 'chargé', 'normalement', 'surveiller', 'radioactiviter', 'contrôler', 'temps', 'exposition', 'techniciens', 'après', 'catastrophe', 'envoyer', 'plusieurs', 'mission', 'tchernobyl', 'avant', 'installer', 'définitivement', 'début', 'autre', 'homme', 'appartenir', 'expédition', 'complexe', 'groupe', 'scientifique', 'venir', 'principal', 'centre', 'recherche', 'atomique', 'union', 'soviétique', 'direction', 'prestigieux', 'institut', 'kourchatov', 'moscou', 'expert', 'consacrer', 'littéral', 'terme', 'apprivoiser', 'monstre', 'mystérieux', 'devenu', 'réacteur', 'précisément', 'devoir', 'localiser', 'tonne', 'combustible', 'nucléaire', 'toujours', 'présent', 'essayer', 'assurer', 'sécurité', 'tâche', 'titan', 'accomplir', 'milieu', 'hostile', 'explosion', 'seul', 'matière', 'radioactif', 'projeter', 'central', 'empoisonner', 'longtemps', 'environ', 'former', 'fameux', 'nuage', 'combustible', 'rester', 'prisonnier', 'carcass', 'dévaster', 'savoir', 'advenir', 'cauchemar', 'physicien', 'tonne', 'matière', 'nucléaire', 'perdition', 'édific', 'détruire', 'risquaient', 'provoquer', 'deuxième', 'catastrophe', 'réaction', 'nucléaire', 'pouvoir', 'redémarrer', 'spontanément', 'entraîner', 'important', 'fuite', 'poisons', 'atmosphèr', 'effet', 'chaleur', 'également', 'risqu', 'syndrome', 'chinois', 'accider', 'populariser', 'américain', 'année', 'lent', 'descente', 'réacteur', 'napp', 'phréatique', 'cinémer', 'enfin', 'partie', 'combustible', 'transformé', 'poussière', 'explosion', 'disséminé', 'bâtiment', 'ruine', 'aller', 'lui', 'échapper', 'impossible', 'savoir', 'totalement', 'exclure', 'entrer', 'bâtiment', 'exhalaient', 'incroyable', 'quantité', 'radiation', 'lendemain', 'accident', 'déverser', 'hélicoptèr', 'millier', 'tonne', 'plomb', 'argil', 'sable', 'étouffer', 'incendie', 'enterrer', 'matière', 'radioactif', 'entre', 'novembre', 'personne', 'travailler', 'construction', 'sarcophage', 'recouvrir', 'le.plu', 'possible', 'bâtiment', 'accidenté', 'intermédiaire', 'grue', 'télécommander', 'surveiller', 'caméra', 'télévision', 'question', 'condition', 'assurer', 'jonction', 'parfait', 'meccano', 'acier', 'béton', 'résultat', 'pansement', 'géer', 'appliquer', 'hermétique', 'trous', 'représenter', 'plusieurs', 'centaine', 'mètre', 'carrer', 'savoir', 'toujours', 'passait', 'intérieur', 'ce', 'cocotte', 'minute', 'virtuel', 'couvercle', 'fuir', 'compliquer', 'pendant', 'construction', 'mètre', 'cube', 'béton', 'répandu', 'édific', 'dévasté', 'écouler', 'voie', 'accès', 'obturant', 'passage', 'recouvrir', 'malgré', 'radioactivité', 'toujours', 'présent', 'homme', 'expédition', 'complexe', 'décident', 'lancer', 'recherche', 'combustible', 'perdre', 'vouloir', 'attendre', 'comment', 'atteindre', 'centre', 'perforer', 'énorme', 'gangue', 'béton', 'matérial', 'fondre', 'entoure', 'traval', 'commencent', 'partir', 'local', 'décontaminer', 'alexandre', 'borovoï', 'patron', 'souvenir', 'parfaitement', 'premier', 'rencontre', 'sacrer', 'combustible', 'rencontre', 'pendant', 'téléphone', 'réveille', 'local', 'remplit', 'brouillard', 'blanchâtre', 'evacuation', 'immédiat', 'ordonne', 'revêtu', 'combinaison', 'masqu', 'pénètre', 'pièce', 'quand', 'arrête', 'laisser', 'passer', 'apercevoir', 'alors', 'brume', 'opaque', 'silhouette', 'sentinell', 'quitter', 'poste', 'partir', 'question', 'répondre', 'soldat', 'officier', 'donner', 'ordre', 'reste', 'encore', 'minute', 'mourir', 'répondre', 'borovoï', 'sentinell', 'enfuir', 'homme', 'expédition', 'désormais', 'souvent', 'lutter', 'contre', 'brouillard', 'riche', 'radioélément', 'cocktail', 'effroyable', 'leur', 'masque', 'protègent', 'pratiquement', 'époque', 'ignorent', 'étonner', 'tousser', 'tout', 'nuit', 'enfin', 'atteignent', 'puits', 'avant', 'accident', 'abritait', 'impatient', 'glissent', 'sond', 'étroit', 'conduire', 'creuser', 'perforatrice', 'mesurer', 'dégâts', 'rencontre', 'combustible', 'disparu', 'mettre', 'alors', 'taupe', 'atomique', 'percer', 'tunnel', 'milieu', 'redoutable', 'radiation', 'retrouver', 'comprendre', 'passer', 'expédition', 'complexe', 'écrire', 'véritable', 'scénario', 'catastroph', 'explosion', 'couvercle', 'réacteur', 'tonne', 'dresser', 'position', 'vertical', 'alors', 'dalle', 'soutènement', 'abaisser', 'mètre', 'quart', 'gigantesque', 'camembert', 'béton', 'purement', 'simplement', 'disparaître', 'contenu', 'combustible', 'graphite', 'fondu', 'mélangé', 'glaise', 'sable', 'plomb', 'déversé', 'hélicoptère', 'répandre', 'forme', 'magma', 'sous-sol', 'bâtiment', 'scientifique', 'découvrir', 'monstrueux', 'langue', 'surnomment', 'patter', 'élépher', 'impossible', 'approcher', 'irradiation', 'mortel', 'commer', 'procurer', 'échantillon', 'borovoï', 'envoie', 'chariot', 'robotiser', 'arracher', 'quelque', 'gramme', 'matière', 'succès', 'alors', 'décide', 'tirer', 'patte', 'géante', 'fusil', 'mitrailleur', 'faire', 'sauter', 'éclat', 'militaire', 'refuser', 'confier', 'leur', 'jamais', 'prêter', 'cependant', 'tireur', 'élite', 'capable', 'selon', 'prouesse', 'atteindre', 'patte', 'élépher', 'revêtir', 'combinaison', 'protection', 'ramper', 'tunnel', 'obscur', 'atmosphèr', 'oppressant', 'arriver', 'lieux', 'tireur', 'élite', 'trembl', 'tellemer', 'tenir', 'fusil', 'enfin', 'calmé', 'attendre', 'traduire', 'toujours', 'absorption', 'supérieur', 'radioactivité', 'exécut', 'arrach', 'échantillon', 'convoité', 'youri', 'kovsar', 'mikhailov', 'participer', 'ce', 'quête', 'infernal', 'comme', 'personne', 'expédition', 'complexe', 'autoriser', 'pénétrer', 'interdit', 'explorer', 'champ', 'ruine', 'traqué', 'combustible', 'renforcé', 'parois', 'menaçaient', 'écrouler', 'procédé', 'travail', 'décontamination', 'essayé', 'fixer', 'poussière', 'radioactif', 'savoir', 'victimer', 'overdose', 'nucléaire', 'kovsar', 'exempl', 'pense', 'corp', 'absorbé', 'environ', 'plaisant', 'mourir', 'avant', 'terrible', 'bénéficier', 'aucun', 'suivi', 'médical', 'sérieux', 'simple', 'visite', 'annuel', 'aucun', 'contrôle', 'concerne', 'strontium', 'plutonium', 'terrible', 'poison', 'radioactif', 'youri', 'appeler', 'lui-même', 'gamma', 'sapiens', 'terrible', 'humour', 'protection', 'estim', 'seul', 'dissection', 'cadavre', 'permettre', 'connaître', 'véritable', 'santé', 'pourquoi', 'rester', 'tchernobyl', 'venir', 'devant', 'nécessité', 'faire', 'catastrophe', 'doute', 'insensible', 'salaire', 'élevé', 'verser', \"aujourd'hui\", 'gagnent', 'environ', 'rouble', 'somme', 'technicien', 'affairent', 'réacteurs', 'fonctionnemer', 'tchernobyl', 'salair', 'moyen', 'union', 'soviétique', 'rouble', 'marché', 'dollar', 'entre', 'roubles', 'cours', 'clandestin', 'revenir', 'représenter', 'environ', 'dollar', 'franc', 'posséder', 'carte', 'officiel', 'liquidateur', 'ainsi', 'appelle', 'hommes', 'travailler', 'entre', 'liquidation', 'accident', 'donne', 'droit', 'privilège', 'territoir', 'union', 'soviétique', 'héroïsme', 'tranquille', 'pensent', 'participer', 'travail', 'nécessaire', 'protégeon', 'espèce', 'humain', 'personne', 'force', 'pénétrer', 'retire', 'leur', 'prime', 'fort', 'irradiation', 'question', 'éloigner', 'resterion', 'payer', 'entrer', 'comme', 'entre', 'religion', 'interdire', 'exercer', 'mortel', 'attraction', 'sentiment', 'celui', 'alpiniste', 'vouloir', 'absolument', 'atteindre', 'sommet', 'invioler', 'habitent', 'tchernobyl', 'kirov', 'immeuble', 'décontaminé', 'radik', 'radiation', 'nourrir', 'aussi', 'ordinaire', 'cantine', 'riche', 'abonder', 'celui', 'ménager', 'moscovite', 'ajouter', 'ultime', 'bravad', 'cèpe', 'cueillir', 'alentour', 'lapin', 'chasser', 'environ', 'contrôle', 'régulièrement', 'appartement', 'expert', 'spécial', 'résultat', 'récemment', 'changer', 'youri', 'devenir', 'radioactif', 'créée', 'milieu', 'dédier', 'réacteur', 'accidenté', 'peuplée', 'construction', 'sarcophage', 'lequel', 'participaient', 'réserviste', 'armée', 'rouge', 'surnommer', 'partisan', 'homme', 'arger', 'attirer', 'nombreux', 'prostitué', 'venue', 'clandestinemer', 'vendre', 'leur', 'charme', 'lui', \"aujourd'hui\", 'pratiquement', 'disparaître', 'alcool', 'officiellement', 'interdire', 'litre', 'vodka', 'introduire', 'cachette', 'atteindre', 'rouble', 'mesure', 'espoirs', 'susciter', 'rumeur', 'tchernobyl', 'alcool', 'protège', 'radiation', 'croire', 'prennent', 'alcool', 'moins', 'atteindre', 'précise', 'affirme', 'expérience', 'animal', 'conforter', 'ce', 'thèse', 'tchernobyl', 'nicolas', 'basil', 'exemple', 'décéder', 'infarctu', 'janvier', 'appartement', 'kirov', 'selon', 'youri', 'personne', 'fortement', 'irradier', 'fréquemment', 'victime', 'crise', 'cardiaque', 'arrête', 'possédés', 'atome', 'élaboré', 'théorie', 'paradoxal', 'faire', 'sourire', 'sujet', 'tragique', 'contraire', 'faible', 'dose', 'affaiblir', 'organisme', 'fort', 'irradiation', 'prolongeraient', 'durer', 'preuve', 'souris', 'laboratoire', 'quatre', 'après', 'solide', 'exposition', 'radioactiviter', 'continuer', 'parcourir', 'interdit', 'moderne', 'sisyph', 'lutter', 'contre', 'ennemi', 'implacable', 'ronge', 'corp', 'béton', 'sacrifice', 'inutile', 'croire', 'alexandre', 'borovoï', 'vacance', 'istra', 'moscou', 'maison', 'repo', 'entourer', 'neiger', \"aujourd'hui\", 'affirme', 'risqu', 'réaction', 'nucléaire', 'central', 'défunt', 'exclure', 'comme', 'éventualité', 'syndrome', 'chinois', 'reste', 'problème', 'poussière', 'radioactif', 'projette', 'régulièrement', 'solution', 'polymère', 'fixer', 'avenir', 'couvercle', 'réacteur', 'effondrer', 'effriter', 'radioactivité', 'échapper', 'trous', 'sarcophage', 'actuellement', 'union', 'soviétique', 'discussion', 'acharner', 'avenir', 'mausoler', 'atomique', 'officiellement', 'ministère', 'energie', 'organis', 'premier', 'audition', 'thèse', 'présence', 'premier', 'faire', 'pelous', 'vert', 'démonter', 'sarcophage', 'réacteur', 'impossible', 'répondre', 'borovoï', 'humain', 'élevé', 'second', 'tchernobyl', 'deuxième', 'propose', 'remplir', 'béton', 'sarcophage', 'existant', 'laisser', 'ce', 'pyramide', 'atomique', 'cadeau', 'génération', 'futur', 'expédition', 'complexe', 'suggère', 'construire', 'autre', 'sarcophage', 'hermétique', 'ce', 'au-dessus', 'premier', 'remplir', 'mousse', 'solidifiant', 'fixerait', 'poussière', 'empêcher', 'démonter', 'réacteur', 'académicien', 'spartak', 'belaiev', 'membre', 'expédition', 'complexe', 'demander', 'succès', 'jusqu', \"'\", 'projet', 'soumettre', 'expert', 'international', 'youri', 'kovsar', 'mikhailov', 'continuer', 'travailler', 'laboratoire', 'géer', 'sûreter', 'autre', 'réacteur', 'homme', 'pensent', 'cacher', 'pass', 'sarcophage', 'tchernobyl', 'emmener', 'victoria', 'ivleva', 'tellemer', 'insister', 'ajoute', 'youri', 'kovsar', 'alors', 'donner', 'quelque']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['tintin', 'espac'], ['suicide', 'robert', 'boulin'], ['pierre', 'contre', 'certitude'], ['otage', 'soudain', 'mercredi'], ['secret', 'planète', 'rouge']]\n",
      "['trois', 'semaine', 'station', 'soviétique', 'jean-loup', 'chrétien', 'premier', 'ouest-européen', 'sortir', 'espace']\n"
     ]
    }
   ],
   "source": [
    "title_tokens = []\n",
    "text_tokens = []\n",
    "\n",
    "## Apply on titles ##\n",
    "\n",
    "for t in news_df.title:\n",
    "    tokens = prepare_text_for_lda(t)\n",
    "    title_tokens.append(tokens)\n",
    "    if random.random() >0.99:\n",
    "        print(tokens)\n",
    "        \n",
    "## Apply on titles ##\n",
    "\n",
    "for t in news_df.text:\n",
    "    tokens = prepare_text_for_lda(t)\n",
    "    text_tokens.append(tokens)\n",
    "    if random.random() >0.999:\n",
    "        print(tokens)\n",
    "        \n",
    "print(title_tokens[:5])\n",
    "print(text_tokens[0][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Perform LDA with Gensim__\n",
    "\n",
    "Fixage arbitraire du nombre de topics, comme on fixerait arbitrairement k dans un k-means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_title = corpora.Dictionary(title_tokens)\n",
    "dic_text = corpora.Dictionary(text_tokens)\n",
    "corpus_title = [dic_title.doc2bow(token) for token in title_tokens]\n",
    "corpus_text = [dic_text.doc2bow(token) for token in text_tokens]\n",
    "\n",
    "pickle.dump(corpus_title, open('corpus_title.pkl', 'wb'))\n",
    "dic_title.save('dic_title.gensim')\n",
    "pickle.dump(corpus_title, open('corpus_text.pkl', 'wb'))\n",
    "dic_text.save('dic_text.gensim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['espace', 'tintin', 'boulin', 'robert', 'suicide', 'certitude', 'contre', 'pierre', 'mercredi', 'otages', 'soudain', 'planète', 'rouge', 'secret', 'ammar', 'forcée', 'marche', 'champion', 'discret', 'olympique', 'quinon', 'algérie', 'faillite', 'sanglante', 'israël', 'ébranlé', 'homme', 'objets', 'ponge', 'besse', 'george', 'pourquoi', 'africain', 'longue', 'mémoire', 'malgré', 'antigone', 'benazir', 'bhutto', 'janvier', 'pakistan', 'soupçons', 'triangle', 'campagne', 'ombre', 'darwin', 'trompé', 'dayan', 'symbole', 'impopulaire', 'leader', 'pérès', 'shimon', 'ainsi', 'exclusif', 'finit', 'phnom', 'témoignage', 'explique', 'simone', 'bureau', 'informatique', 'livré', 'jérusalem', 'nouveau', 'seuls', 'chopinet', 'express', 'retrouve', 'touvier', 'tutelle', 'agent', 'fausses', 'france', 'star', 'vrais', 'complexe', 'pinochet', 'challenger', 'drame', 'beineix', 'coluche', 'nourricier', 'marchandage', 'terrorisme', 'enquête', 'reprend', 'espagne', 'goytisolo', 'sarrasin', 'attentat', 'blanco', 'carrero', 'décembre', 'avortement', 'médecins', 'peine', 'hors-la-loi', 'extraits', 'guerre', 'kippour', 'autre', 'front', 'arafat', 'intronisé', 'yasser', 'femmes', 'pouvoir', 'hearst', 'nouvel', 'patricia', 'épisode', 'appel', 'ambulance', 'feuilleton', 'dibie', 'ethnologue', 'pascal', 'tribu', 'douce', 'manifestation', 'novembre', 'retour', 'tchécoslovaquie', 'pologne', 'solidarité', 'bocaux', 'énigme', 'destin', 'sacré', 'capitalisme', 'ouvre', 'délit', 'fuite', 'ozone', 'chadli', 'joker', 'faucon', 'têtes', 'future', 'noiret', 'sacrée', 'tavernier', 'union', 'jours', 'pékin', 'tiananmen', 'droits', 'famine', 'château', 'havel', 'prison', 'vaclav', 'banqueroute', 'mathieu', 'selon', 'panne', 'revanche', 'serbes', 'chargeurs', 'réunis', 'jeune', 'envers', 'paradis', 'chine', 'liberté', 'typhon', 'enfant', 'saint-laurent', 'triste', 'burguéra', 'méthode', 'baton', 'carotte', 'couette', 'degrés', 'carthage', 'rêver', 'intemporel', 'changer', 'crise', 'japon', 'paysans', 'ploucs', 'carrefour', 'science', 'conclusion', 'heureuses', 'liaison', 'croisade', 'mafia', 'papale', 'guignols', 'au-dessus', 'force', 'concertee', 'impossible', 'recession', 'reponse', 'hamas', 'occupe', 'territoires', 'haine', 'jug', 'mille', 'normale', 'nuits', 'chiffres', 'querelle', 'masquent', 'tarifs', 'transparents', 'envahie', 'europe', 'chomage', 'capital', 'comment', 'gerer', 'image', 'mammifère', 'étonnant', 'politique', 'socialistes', 'droit', 'limit', 'amour', 'hommes', 'reperes', 'quelle', 'étrangère', 'eurosceptiques', 'montée', 'terre', 'atmosphere', 'drôle', 'progrès', 'histoire', 'négociation', 'secrète', 'carton', 'jaune', 'tapie', 'agence', 'associe', 'consigny', 'directeur', 'lundi', 'octobre', 'opera', 'publicite', 'thierry', 'bazar', 'economie', 'russie', 'compact', 'irest', 'charlot', 'nappes', 'parler', 'pouvaient', 'idylle', 'staccato', 'bourse', 'gérer', 'change', 'liberte', 'presse', 'courrier', 'lecteurs', 'spirituel', 'credit', 'encore', 'lyonnais', 'tuile', 'changé', 'découvertes', 'indispensable', 'laser', 'hermon', 'michel', 'utile', 'négliger', 'bernard', 'madagascar1853', 'justice', 'sociale', 'orgueil', 'franchise', 'heure', 'vérité', 'chaos', 'morts', 'cette', 'devon', 'victoire', 'afrique', 'continent', 'miettes', 'céline', 'l.-f.', 'amérique', 'nouvelle', 'capitaine', 'grand', 'cameroun', 'deuxième', 'cent', 'métier', 'cherchent', 'patron', 'repreneur', 'influence', 'debat', 'interdit', 'interet', 'joyaux', 'éternels', 'guiloineau', 'mandela', 'nelson', 'implacable', 'santé', 'intempéries', 'tempérées', 'carnage', 'révélations', 'renseignements', 'utiles', 'course', 'decodeurs', 'méphisto', 'evaluation', 'petit', 'sortie', 'boris', 'eltsine', 'maladie', 'dollar', 'baisse', 'gandois', 'salaires', 'dijon', 'fêtards', 'italienne', 'surmontera', 'pantomime', 'port-au-prince', 'libraires', 'impitoyable', 'univers', 'amère', 'sud-ouest', 'trêve', 'chiffre', 'retenir', 'embellie', 'flaconnage', 'blessée', 'dresde', 'licencies', 'priorite', 'reembauchage', 'salary', 'fêtard', 'vague', 'diplomate', 'quotidienne', 'failli', 'perdre', 'excès', 'royaumes', 'américain', 'etape', 'atteint', 'minitel', 'diamants', 'fantômes', 'centrale', 'combourg', 'cerise', 'elles', 'lèvres', 'chant', 'dieux', 'yakoutie', 'astérix', 'theodore', 'zeldin', 'diluée', 'epopee', 'eurotunnel', 'étreinte', 'franglais', 'lexique', 'managerial', 'eternelle', 'lumiere', 'presque', 'aéropostale', 'bouilloux', 'lafont', 'marcel', 'chante', 'toute', 'calvet', 'epuisant', 'imprecateur', 'choix', 'paris', 'bananes', 'nabab', 'island', 'kangaroo', 'revivre', 'combien', 'emploi', 'offres', 'stabilisation', 'comptes', 'légende', 'ancien', 'fremissement', 'essec', 'mardis', 'suisse', 'feuille', 'impot', 'suivez', 'départ', 'apollo', 'après', 'monde', 'juillet', 'désormais', 'misent', 'nigérians', 'vivra', 'peuple', 'répression', 'alarme', 'supprimer', 'budapest', 'parle', 'sartre', 'doute', 'semaine', 'densité', 'implantation', 'rebelle', 'clef', 'psychanalyse', 'français', 'bollardière', 'combat', 'général', 'gagne', 'gourion', 'point', 'française', 'léone', 'mazurat', 'victime', 'complexité', 'edgar', 'morin', 'economiste', 'tirez', 'chorégies', 'enjeu', 'médias', 'entre', 'quell', 'racine', 'saveurs', 'senteurs', 'chantiers', 'septennat', 'véritable', 'devant', 'inégaux', 'plaisir', 'syndic', 'apres', 'abbas', 'algerienne', 'ferhat', 'utopie', 'cable', 'menace', 'satelli', 'camp', 'horreur', 'nazi', 'tours', 'deficit', 'banlieues', 'vivier', 'suédoises', 'humanitaire', 'janina', 'ochojska', 'pédagogie', 'financier', 'client', 'culture', 'flambe', 'noumea', 'paysan', 'volant', 'verbatim', 'mutation', 'nostra', 'immigres', 'affaire', 'révélons', 'budgetaire', 'controle', 'classement', 'charismatique', 'ministre', 'premier', 'québec', 'mensonge', 'prisonnier', 'dejouany', 'examen', 'berlin', 'doigts', 'cadre', 'fiers', 'partiel', 'temp', 'charles', 'maternité', 'professeur', 'rudigoz', 'désir', 'lorsque', 'paraît', 'rebonds', 'chirac', 'miroir', 'association', 'collimateur', '-nous', 'bien-être', 'combine', 'milliard', 'langage', 'exposition', 'sortir', 'leçons', 'société', 'émissions', 'film', 'communisme', 'jurassic', 'lénine', 'colon', 'préoccupent', 'khmer', 'bagarre', 'bebes', 'budget', 'coup', 'policiers', 'écrans', 'interesse', 'supporter', 'esclave', 'martyr', 'demenager', 'suffit', 'sureffectifs', 'syndicats', 'veulent', 'petite', 'phrase', 'gustatif', 'voyage', 'françoise', 'giroud', 'tigre', 'mauvais', 'signaux', 'brevete', 'vampire', 'italie', 'moral', 'ordre', 'guide', 'quand', 'ville', 'elysée', 'étranger', 'antimissile', 'orbite', 'concert', 'virtuels', 'héritage', 'rabin', 'institutionnelle', 'trace', 'badinter', 'eichmann', 'procès', 'kennedy', 'contingent', 'o.a.s.', 'programmme', 'heures', 'américains', 'barka', 'récit', 'témoin', 'chinoise', 'culturelle', 'regard', 'révolution', 'chapeau', 'envoyés', 'spéciaux', 'jean-marie', 'vraiment', 'lama', 'steppe', 'bonne', 'volonté', 'innovatrices', 'vitamine', 'all', 'franc', 'jusqu', 'ponction', 'pourrait', 'defient', 'gouvernement', 'tourisme', 'accor', 'stratégie', \"1'europe\", 'parias', 'tziganes', 'engrenage', 'medecin', 'salle', 'irremplaçable', 'mendès', 'meurtrière', 'accord', 'renault', 'volvo', 'courant', 'internationaliste', 'elizabeth', 'taylor', 'competitivite', 'prefere', 'productivite', 'directrice', 'trois', 'costume', 'marzotto', 'philippe', 'portrait', 'séguin', 'discipline', 'inflation', 'lesfrancais', 'quiachange', 'vivre', 'mince', 'responsabilité', 'bercy', 'laisse', 'marque', 'commission', 'offensive', 'orient', 'pacifique', 'proche', 'empire', 'levant', 'soleil', 'train', 'exode', 'route', 'allemande', 'crisis', 'nouvelles', 'regles', 'sauvage', 'gage', 'tueur', 'attendent', 'islamistes', 'poids', 'golda', 'espoir', 'illusion', 'comme', 'détective', 'marguerite', 'moïse', 'opération', 'demain', 'kanaky', 'tjibaou', 'honte', 'vallée', 'lettre', 'jamais', 'yamit', 'cactus', 'sinaï', 'autres', 'jean-pierre', 'écart', 'acteur', 'désengagé', 'avril', 'financer', 'retraite', 'ambiguïtés', 'holmes', 'moines', 'sherlock', 'double', 'désastreux', 'moins', 'assassinat', 'bertrand', 'cambodge', 'goulag', 'bédouins', 'glaces', 'commando', 'suite', 'henry', 'patrick', 'lrgoun', 'décider', 'euthanasie', 'médecin', 'schwartzenberg', 'condamnation', 'appeler', 'augusto', 'cessa', 'verdict', 'shamir', 'soldat', 'seconde', 'bibliothèque', 'dumberto', 'arrive', 'pauvres', 'génocide', 'burgos', 'biafra', \"jusqu'\", 'éternité', 'parachutiste', 'guérilleros', 'téhéran', 'chute', 'strasbourg', 'cirque', 'clignancourt', 'mesrine', 'giscard', 'espagnole', 'filière', 'goldman', 'lendemains', 'boulets', 'européenne', 'khomeini', 'occident', 'trembler', 'personne', 'dernier', 'dialogue', 'bombe', 'guêpier', 'frontière', 'demission', 'dessous', 'hopitaux', 'attaque', 'corruption', 'juppe', 'confession', 'policier', 'chien', 'management', 'consulter', 'francais', 'danoise', 'mariages', 'internationale', 'multiplier', 'pression', 'cardin', 'galaxie', 'mode', 'consommation', 'reprise', 'street', 'cinéma', 'grain', 'sable', 'vatican', 'encyclopedie', 'rupture', 'immigrés', 'leurs', 'diana', 'gloire', 'decollage', 'pauvre', 'armateur', 'barraquand', 'barreur', 'bollore', 'privilegier', 'syndicat', 'bien-naître', 'islande', 'devenue', 'expansion', 'fevrier', 'generation', 'precarite', 'dictature', 'narco', 'cannabis', 'toxique', 'allege', 'chambre', 'ecolo', 'energie', 'sydney', 'siens', 'toujours', 'trahi', 'complices', 'elitisme', 'populisme', 'armée', 'decus', 'patronat', 'bretagne', 'grande', 'vendre', 'cohen', 'daniel', 'flammarion', 'nation', 'page', 'pauvretes', 'richesse', 'botanique', 'cimes', 'tropicales', 'clément', 'entourloupe', 'judiciaire', 'publication', 'aquitaine', 'benie', 'brent', 'jaffre', 'énigmes', 'empoisonné', 'parquet', 'tardif', 'libre', 'modèles', 'maître', 'roseraie', 'allemagne', 'réunie', 'humeur', 'salaud', 'touche', 'frere', 'galere', 'jacques', 'saade', 'lanterne', 'francemane', 'industrie', 'apprend', 'atlantique', 'alcatel', 'patte', 'thomson', 'velours', 'antirigueur', 'argentine', 'menem', 'revolte', 'attente', 'disaient', 'expatriez', 'bilan', 'chapier', 'laisser', 'singulier', 'pedro', 'murmure', 'bator', 'ouest', 'oulan', 'délire', 'initié', 'umberto', 'libéria', 'national', 'menacé', 'parretti', 'masque', 'plume', 'westlake', 'parti', 'pluralisme', 'adolf', 'chancelier', 'hitler', 'rescapés', 'santa', 'enfants', 'juifs', 'milliers', 'financement', 'éclaté', 'apocalypse', 'folle', 'odyssée', 'femme', 'fusil', 'journal', 'dix-huit', 'islam', 'restez', 'veille', 'bousquet', 'vraie', 'bongo', 'montre', 'février', 'palestiniens', 'réfugiés', 'reculer', 'telecom', 'aguichante', 'argent', 'rideau', 'effort', 'madame', 'prendre', 'calculez', 'chef', 'devriez', 'gagner', 'service', 'secteur', 'telecommunication', 'jospin', 'fausse', 'honkong', 'vision', 'castries', 'henri', 'regner', 'royaume', 'commerce', 'exterieur', 'reparti', 'americaines', 'negociations', 'europeenne', 'seattle', 'confidentiel', 'réponse', 'maltraitance', 'principe', 'monument', 'remords', 'fiscal', 'surplus', 'conseillera', 'waigel', 'croissance', 'mondiale', 'command', 'etranger', 'profite', 'indépendanse', 'tibet', 'législatives', 'résultat', 'fiscalite', 'promet', 'encheres', 'trouble', 'deutsche', 'sanctionne', 'telekom', 'balladur', 'devedjian', 'donne', 'hommes-femmes', 'malentendu', 'cinema', 'achetez', 'grossistes', 'sicav', 'années', 'avoir', 'olivier', 'rolin', 'agriculture', 'americains', 'raboter', 'affrontent', 'arnault', 'internet', 'pinault', 'emprunt', 'financera', 'relance', 'business', 'mediterranee', 'orientale', 'industrielle', 'production', 'rechute', 'septembre', 'publique', 'gerhard', 'impots', 'massivement', 'schroder', 'abecedaire', 'amiante', 'revolution', 'silencieuse', 'choses', 'sport', 'lionel', 'parie', 'accelere', 'decelerent', 'menage', 'record', 'baudis', 'défis', 'maison', 'restauration', 'chrono', 'convertit', 'reseau', 'antimondialisation', 'exporte', 'chrétiens', 'visage', 'joyeux', 'famille', 'confirme', 'emplois', 'suppression', 'blair', 'licencier', 'tiens', 'oestrich', 'suède', 'paritarisme', 'championne', 'banque', 'public', 'epanouissement', 'fleurs', 'plein', 'astres', 'cayrol', 'publica', 'roland', 'marocain', 'pleurent', 'protecteur', 'finale', 'rugby', 'craindre', 'petrolier', 'prochain', 'troisieme', 'platanisme', 'ravage', 'decennie', 'meilleur', 'vent', 'medef', 'cerveau', 'dependance', 'molecule', 'reduire', 'toxicomanie', 'aerospatiale', 'couac', 'fusion', 'matra', 'activite', 'hausse', 'ralentie', 'enflamme', 'istambul', 'résultats', 'perdue', 'poésie', 'retrouvée', 'action', 'bonnes', 'rentables', 'bijou', 'chaumet', 'horloges', 'offre', 'danger', 'monsieur', '-vous', 'préparez', 'alstom', 'revers', 'siemens', 'subit', 'taiwan', 'abusifs', 'licenciements', 'taxer', 'commerciaux', 'enjeux', 'gouvernementales', 'mesures', 'inquiet', 'preparation', 'reunion', 'lyonnaise', 'rappelee', 'tractebel', 'mobile', 'salon', 'aubry', 'excedent', 'contes', 'irlande', 'selftrade', 'recentrage', 'platz', 'potsdamer', 'européennes', 'ténébreuse', 'fruit', 'verts', 'algerie', 'mend', 'ingénieurs', 'mobilité', 'élèves', 'camargue', 'oustique', 'coûteuses', 'négligences', 'allie', 'bebear', 'paribas', 'aides', 'europeennes', 'recoivent', 'region', 'balise', 'chemin', 'obstacle', 'pourtant', 'débriefing', 'creent', 'departements', 'equipe', 'mondial', 'xavier', 'partner', 'controversé', 'vaccin', 'frèches', 'surprise', 'basculé', 'bouygues', 'brother', 'consultant', 'etait', 'ailleurs', 'krugman', 'ballon', 'micro', 'apôtre', 'droite', 'extrême', 'attitude', 'routards', 'bienfaits', 'cohabitation', 'coffret', 'decouvertes', 'gallimard', 'havas', 'interactive', 'larousse', 'polemique', 'prevision', 'grace', 'rayonne', 'panama', 'strategie', 'ronflement', 'spleen', 'corée', 'idéale', 'touristique', 'climat', 'deteriore', 'misere', 'tortionnaire', 'chantant', 'demissionne', 'enfance', 'pâture', 'fonctionnaires', 'nippon', 'terreur', 'chronologie', 'democratie', 'anonyme', 'passionnément', 'naître', 'indomptable', 'shanghai', 'fache', 'gastronomie', 'contrer', 'invoque', 'ciblee', 'reduction', 'reflechissons', 'examine', 'ultrasons', 'amiens', 'capitale', 'téléphonie', 'multimédia', 'position', 'prend', 'schuman', 'conduit', 'houphouët', 'bénin', 'jouent', 'regrets', 'serviteur', 'vichy', 'fatiguée', 'parrain', 'musique', 'notte', 'croatie', 'déchirure', 'enfer', 'kurdistan', 'oasis', 'contrôlé', 'dérapage', 'madrid', 'promesses', 'charité', 'ordonnée', 'coeur', 'descente', 'bluff', 'bossi', 'gadget', 'prenait', 'candidats', 'embauche', 'ordinateur', 'sélection', 'couleurs', 'press', 'primaires', 'calvaire', 'scientologue', 'inégal', 'netanyahu', 'arrivee', 'etats', 'washington', 'rwanda', 'privation', 'verite', 'néerlandais', 'cotte', 'mailles', 'après-aïdid', 'espoirs', 'somalie', 'ténus', 'chancellerie', 'inquiete', 'marches', 'caisse', 'geste', 'passant', 'profanateurs', 'figaro', 'heritiers', 'guérir', 'oncle', 'otage', 'defaisance', 'mexicaine', 'cherchez', 'indifférence', 'tollé', 'folles', 'hollywood', 'doter', 'elementaire', 'gestion', 'locale', 'organismes', 'frontieres', 'passe', 'coupable', 'forum', 'skoda', 'exploitez', 'johnny', 'malheur', 'police', 'prudence', 'reforme', 'progresse', 'seule', 'finance', 'holding', 'nettoyage', 'operation', 'cache', 'dingues', 'tendance', 'tongs', 'économie', 'lescure', 'moteur', 'nethold', 'fragiles', 'jeunes', 'rêves', 'syriens', 'consommateurs', 'marketing', 'reconcilient', 'yaourt', 'belgique', 'aussi', 'cancer', 'cigarette', 'gene', 'bénéfices', 'hezbollah', 'disney', 'television', 'harcèlement', 'prion', 'savoir', 'botte', 'hauts', 'pasqua', 'replie', 'seine', 'dingue', 'indice', 'nikkei', 'piégée', 'avocats', 'cabinet', 'crible', 'pass', 'vivants', 'farines', 'poissons', 'viande', 'zapper', 'convoite', 'wireless', 'marilyn', 'monroe', 'réussir', 'confus', 'dossier', 'charonne', 'massacre']\n"
     ]
    }
   ],
   "source": [
    "print([k for k in dic_title.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topics\n",
      "(0, '0.020*\"retour\" + 0.010*\"encore\" + 0.010*\"heure\" + 0.007*\"berlin\"')\n",
      "(1, '0.010*\"homme\" + 0.010*\"affaire\" + 0.010*\"boulin\" + 0.007*\"cessa\"')\n",
      "(2, '0.007*\"france\" + 0.007*\"comment\" + 0.007*\"jospin\" + 0.007*\"victoire\"')\n",
      "(3, '0.020*\"express\" + 0.010*\"drame\" + 0.007*\"monde\" + 0.007*\"marche\"')\n",
      "(4, '0.009*\"robert\" + 0.009*\"suicide\" + 0.009*\"heures\" + 0.009*\"express\"')\n",
      "(5, '0.009*\"allemagne\" + 0.009*\"petit\" + 0.009*\"chute\" + 0.006*\"semaine\"')\n",
      "(6, '0.014*\"france\" + 0.006*\"décembre\" + 0.006*\"nouvelle\" + 0.006*\"mode\"')\n",
      "(7, '0.011*\"pouvoir\" + 0.007*\"jours\" + 0.007*\"guerre\" + 0.007*\"extraits\"')\n",
      "(8, '0.010*\"europe\" + 0.007*\"pierre\" + 0.007*\"patron\" + 0.007*\"patrick\"')\n",
      "(9, '0.018*\"contre\" + 0.015*\"france\" + 0.012*\"francais\" + 0.009*\"europe\"')\n",
      "\n",
      "Topics\n",
      "(0, '0.008*\"affaire\" + 0.008*\"suite\" + 0.008*\"ombre\" + 0.008*\"deutsche\"')\n",
      "(1, '0.009*\"express\" + 0.006*\"henry\" + 0.006*\"reseau\" + 0.006*\"patron\"')\n",
      "(2, '0.010*\"suicide\" + 0.007*\"israël\" + 0.007*\"jospin\" + 0.007*\"pierre\"')\n",
      "(3, '0.034*\"france\" + 0.012*\"express\" + 0.009*\"pouvoir\" + 0.009*\"nouvelle\"')\n",
      "(4, '0.015*\"contre\" + 0.009*\"semaine\" + 0.009*\"europe\" + 0.006*\"homme\"')\n",
      "(5, '0.007*\"français\" + 0.007*\"arafat\" + 0.007*\"japon\" + 0.007*\"croissance\"')\n",
      "(6, '0.010*\"italie\" + 0.010*\"japon\" + 0.006*\"marche\" + 0.006*\"crise\"')\n",
      "(7, '0.009*\"fantômes\" + 0.009*\"france\" + 0.009*\"jérusalem\" + 0.006*\"guerre\"')\n",
      "(8, '0.016*\"retour\" + 0.009*\"robert\" + 0.006*\"bourse\" + 0.006*\"algérie\"')\n",
      "(9, '0.019*\"histoire\" + 0.007*\"retraite\" + 0.007*\"europe\" + 0.007*\"seattle\"')\n"
     ]
    }
   ],
   "source": [
    "# get the topics! \n",
    "\n",
    "NUM_TOPICS = 10\n",
    "\n",
    "for (corpus, dictionary) in [(corpus_title, dic_title), (corpus_text, dic_text)]:\n",
    "    print('\\nTopics')\n",
    "    ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = NUM_TOPICS, id2word=dictionary, passes=15)\n",
    "    ldamodel.save('model5.gensim')\n",
    "    topics = ldamodel.print_topics(num_words=4)\n",
    "    for topic in topics:\n",
    "        print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to be continued\n",
    "# https://towardsdatascience.com/topic-modelling-in-python-with-nltk-and-gensim-4ef03213cd21"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Analysis__\n",
    "\n",
    "* at first sight, key words associated to the same topic do not always have much in common \n",
    "* some of the words used to define topics do not seem like relevant, high level key words\n",
    "* maybe the corpus is to small or to heterogeneous to obtain relevant topics this way!\n",
    "\n",
    "--> *Improvement ideas*\n",
    "- try clustering methods and put a word on the topics\n",
    "- improve dictionary\n",
    "- improve this first LDA model (more ideas [here](https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/))\n",
    "- get more data on specific topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Clustering \n",
    "\n",
    "K-Means clustering and DBSCAN with tf-idf representation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Build a tf-idf matrix to represent the corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Naive approach based on word count__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(726, 43869)\n"
     ]
    }
   ],
   "source": [
    "#instantiate CountVectorizer() \n",
    "cv=CountVectorizer() \n",
    " \n",
    "# this steps generates word counts for the words in your docs \n",
    "word_count_vector=cv.fit_transform(news_df.text)\n",
    "\n",
    "print(word_count_vector.shape)\n",
    "# Houston, we have a dimensionality problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idf_weights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>de</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>la</th>\n",
       "      <td>1.022254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>le</th>\n",
       "      <td>1.036419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>des</th>\n",
       "      <td>1.047896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>les</th>\n",
       "      <td>1.052235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>et</th>\n",
       "      <td>1.053685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>un</th>\n",
       "      <td>1.074214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>en</th>\n",
       "      <td>1.077181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>du</th>\n",
       "      <td>1.081649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>une</th>\n",
       "      <td>1.124338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qui</th>\n",
       "      <td>1.144795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>est</th>\n",
       "      <td>1.147980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pour</th>\n",
       "      <td>1.149576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>au</th>\n",
       "      <td>1.154380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dans</th>\n",
       "      <td>1.159207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>par</th>\n",
       "      <td>1.164057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>que</th>\n",
       "      <td>1.177108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sur</th>\n",
       "      <td>1.183698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>il</th>\n",
       "      <td>1.198686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pas</th>\n",
       "      <td>1.252101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ce</th>\n",
       "      <td>1.259206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>plus</th>\n",
       "      <td>1.269958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>se</th>\n",
       "      <td>1.280828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mais</th>\n",
       "      <td>1.293660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ne</th>\n",
       "      <td>1.295507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avec</th>\n",
       "      <td>1.314164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qu</th>\n",
       "      <td>1.316049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>son</th>\n",
       "      <td>1.342820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aux</th>\n",
       "      <td>1.360415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sont</th>\n",
       "      <td>1.415140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>complexus</th>\n",
       "      <td>6.895779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mirella</th>\n",
       "      <td>6.895779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mirassou</th>\n",
       "      <td>6.895779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>miroiter</th>\n",
       "      <td>6.895779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>compils</th>\n",
       "      <td>6.895779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>compilent</th>\n",
       "      <td>6.895779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>compilation</th>\n",
       "      <td>6.895779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mitraillée</th>\n",
       "      <td>6.895779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>compensaient</th>\n",
       "      <td>6.895779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>compensant</th>\n",
       "      <td>6.895779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mitraille</th>\n",
       "      <td>6.895779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mitraillages</th>\n",
       "      <td>6.895779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mitraillage</th>\n",
       "      <td>6.895779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mitoyen</th>\n",
       "      <td>6.895779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mitonné</th>\n",
       "      <td>6.895779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mitonne</th>\n",
       "      <td>6.895779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>miti</th>\n",
       "      <td>6.895779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mites</th>\n",
       "      <td>6.895779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>miséricorde</th>\n",
       "      <td>6.895779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>compensera</th>\n",
       "      <td>6.895779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>compenseraient</th>\n",
       "      <td>6.895779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>compenserait</th>\n",
       "      <td>6.895779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mistes</th>\n",
       "      <td>6.895779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mister</th>\n",
       "      <td>6.895779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>miste</th>\n",
       "      <td>6.895779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>missives</th>\n",
       "      <td>6.895779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>missive</th>\n",
       "      <td>6.895779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>compensés</th>\n",
       "      <td>6.895779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>miradors</th>\n",
       "      <td>6.895779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>œuvre</th>\n",
       "      <td>6.895779</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>43869 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                idf_weights\n",
       "de                 1.000000\n",
       "la                 1.022254\n",
       "le                 1.036419\n",
       "des                1.047896\n",
       "les                1.052235\n",
       "et                 1.053685\n",
       "un                 1.074214\n",
       "en                 1.077181\n",
       "du                 1.081649\n",
       "une                1.124338\n",
       "qui                1.144795\n",
       "est                1.147980\n",
       "pour               1.149576\n",
       "au                 1.154380\n",
       "dans               1.159207\n",
       "par                1.164057\n",
       "que                1.177108\n",
       "sur                1.183698\n",
       "il                 1.198686\n",
       "pas                1.252101\n",
       "ce                 1.259206\n",
       "plus               1.269958\n",
       "se                 1.280828\n",
       "mais               1.293660\n",
       "ne                 1.295507\n",
       "avec               1.314164\n",
       "qu                 1.316049\n",
       "son                1.342820\n",
       "aux                1.360415\n",
       "sont               1.415140\n",
       "...                     ...\n",
       "complexus          6.895779\n",
       "mirella            6.895779\n",
       "mirassou           6.895779\n",
       "miroiter           6.895779\n",
       "compils            6.895779\n",
       "compilent          6.895779\n",
       "compilation        6.895779\n",
       "mitraillée         6.895779\n",
       "compensaient       6.895779\n",
       "compensant         6.895779\n",
       "mitraille          6.895779\n",
       "mitraillages       6.895779\n",
       "mitraillage        6.895779\n",
       "mitoyen            6.895779\n",
       "mitonné            6.895779\n",
       "mitonne            6.895779\n",
       "miti               6.895779\n",
       "mites              6.895779\n",
       "miséricorde        6.895779\n",
       "compensera         6.895779\n",
       "compenseraient     6.895779\n",
       "compenserait       6.895779\n",
       "mistes             6.895779\n",
       "mister             6.895779\n",
       "miste              6.895779\n",
       "missives           6.895779\n",
       "missive            6.895779\n",
       "compensés          6.895779\n",
       "miradors           6.895779\n",
       "œuvre              6.895779\n",
       "\n",
       "[43869 rows x 1 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tf-idf with all the words\n",
    "tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True) \n",
    "tfidf_transformer.fit(word_count_vector)\n",
    "\n",
    "# print idf values \n",
    "df_idf = pd.DataFrame(tfidf_transformer.idf_, index=cv.get_feature_names(),columns=[\"idf_weights\"]) \n",
    " \n",
    "# sort ascending \n",
    "df_idf.sort_values(by=['idf_weights'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to select the words with highest tf-idf count."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Tf-Idf vectorizer with constraints__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit tf-idf on all texts with constraints on document frequency and number of tokens\n",
    "tfidf_vectorizer=TfidfVectorizer(max_df = 700, max_features=500, smooth_idf=True,use_idf=True) \n",
    "X = tfidf_vectorizer.fit_transform(news_df.text) # X is a matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['000', '10', '100', '11', '12', '15', '16', '17', '1990', '1997', '20', '30', '31', '50', '500', '60', 'abord', 'accord', 'action', 'affaire', 'affaires', 'afin', 'afrique', 'ai', 'aide', 'ailleurs', 'ainsi', 'ait', 'algérie', 'allemagne', 'alors', 'américain', 'américaine', 'américains', 'amérique', 'an', 'ancien', 'année', 'années', 'ans', 'août', 'après', 'argent', 'armes', 'armée', 'assez', 'au', 'aucun', 'aucune', 'aujourd', 'aura', 'aurait', 'aussi', 'autant', 'autour', 'autre', 'autres', 'aux', 'avaient', 'avait', 'avant', 'avec', 'avenir', 'avoir', 'avons', 'avril', 'bas', 'beaucoup', 'bien', 'bon', 'bonne', 'bout', 'camp', 'campagne', 'capitale', 'car', 'cas', 'cause', 'ce', 'cela', 'celle', 'celui', 'centre', 'certaines', 'certains', 'ces', 'cet', 'cette', 'ceux', 'chacun', 'chaque', 'chef', 'chez', 'chine', 'chose', 'chômage', 'ci', 'cinq', 'comme', 'comment', 'commerce', 'commission', 'compte', 'conseil', 'contre', 'contrôle', 'corps', 'coup', 'coups', 'cours', 'crise', 'côté', 'dans', 'demande', 'depuis', 'dernier', 'dernière', 'des', 'deux', 'devant', 'devenu', 'dire', 'directeur', 'dirigeants', 'discours', 'dit', 'dix', 'doit', 'dollars', 'donc', 'donner', 'dont', 'doute', 'droit', 'droite', 'droits', 'du', 'dès', 'début', 'décembre', 'défense', 'déjà', 'désormais', 'effet', 'elle', 'elles', 'emploi', 'emplois', 'en', 'encore', 'enfants', 'enfin', 'ensemble', 'ensuite', 'entre', 'entreprise', 'entreprises', 'est', 'et', 'etat', 'etats', 'eu', 'europe', 'européen', 'européenne', 'eux', 'exemple', 'explique', 'express', 'face', 'faire', 'fait', 'famille', 'faut', 'façon', 'femme', 'femmes', 'feu', 'fils', 'fin', 'fois', 'font', 'force', 'forces', 'fort', 'france', 'francs', 'français', 'française', 'françois', 'fut', 'garde', 'gauche', 'gaulle', 'gens', 'gouvernement', 'grand', 'grande', 'grandes', 'grands', 'groupe', 'grâce', 'guerre', 'général', 'haut', 'heure', 'heures', 'histoire', 'homme', 'hommes', 'hui', 'huit', 'ici', 'idée', 'il', 'ils', 'image', 'indépendance', 'intérieur', 'israël', 'jacques', 'jamais', 'janvier', 'je', 'jean', 'jeune', 'jeunes', 'jour', 'jours', 'juifs', 'juillet', 'juin', 'jusqu', 'justice', 'jérusalem', 'kennedy', 'le', 'les', 'leur', 'leurs', 'liberté', 'lieu', 'loi', 'loin', 'long', 'longtemps', 'lors', 'lorsqu', 'lorsque', 'lui', 'là', 'ma', 'mai', 'main', 'maintenant', 'mais', 'maison', 'majorité', 'mal', 'malgré', 'marché', 'mars', 'matin', 'me', 'membres', 'mettre', 'mieux', 'militaire', 'militaires', 'milliards', 'millions', 'ministre', 'ministère', 'mis', 'mise', 'moi', 'moins', 'mois', 'moment', 'mon', 'monde', 'mort', 'morts', 'mouvement', 'moyens', 'même', 'mêmes', 'national', 'nationale', 'ne', 'ni', 'nom', 'nombre', 'non', 'nord', 'nos', 'notamment', 'notre', 'nous', 'nouveau', 'nouveaux', 'nouvelle', 'novembre', 'nuit', 'octobre', 'on', 'ont', 'opinion', 'opération', 'or', 'ordre', 'organisation', 'ou', 'oui', 'où', 'paix', 'palestiniens', 'par', 'parce', 'parfois', 'paris', 'parler', 'parmi', 'part', 'parti', 'partie', 'partir', 'pas', 'passe', 'passer', 'passé', 'patron', 'pays', 'peine', 'pendant', 'personne', 'personnes', 'petit', 'petite', 'peu', 'peuple', 'peur', 'peut', 'peuvent', 'pierre', 'place', 'plan', 'plupart', 'plus', 'plusieurs', 'plutôt', 'point', 'police', 'politique', 'politiques', 'population', 'porte', 'possible', 'pour', 'pourquoi', 'pourrait', 'pourtant', 'pouvoir', 'premier', 'premiers', 'première', 'prendre', 'presque', 'presse', 'pris', 'prix', 'problème', 'proche', 'produits', 'projet', 'près', 'président', 'pu', 'public', 'publique', 'puis', 'père', 'qu', 'quand', 'quatre', 'que', 'quelque', 'quelques', 'question', 'qui', 'quoi', 'raison', 'rapport', 'responsable', 'reste', 'retour', 'rien', 'risque', 'rouge', 'rue', 'régime', 'région', 'république', 'révolution', 'rôle', 'sa', 'saint', 'sait', 'salariés', 'sans', 'savoir', 'se', 'secrétaire', 'selon', 'semaine', 'semaines', 'semble', 'sens', 'septembre', 'sera', 'serait', 'seront', 'service', 'services', 'ses', 'seul', 'seule', 'seulement', 'si', 'situation', 'six', 'siècle', 'société', 'soit', 'soldats', 'sommes', 'son', 'sont', 'sous', 'souvent', 'soviétique', 'succès', 'sud', 'suis', 'sur', 'surtout', 'système', 'sécurité', 'sûr', 'tant', 'tard', 'taux', 'tel', 'temps', 'terme', 'terre', 'toujours', 'tour', 'tous', 'tout', 'toute', 'toutes', 'travail', 'trente', 'trois', 'trop', 'très', 'tête', 'un', 'une', 'union', 'unis', 'va', 'vers', 'veut', 'vie', 'vient', 'vieux', 'ville', 'vingt', 'vite', 'voilà', 'voir', 'voit', 'voix', 'vont', 'vous', 'vrai', 'vu', 'yeux', 'ça', 'économie', 'économique', 'également', 'époque', 'étaient', 'était', 'état', 'été', 'être']\n",
      "(726, 500)\n"
     ]
    }
   ],
   "source": [
    "# take a look at the result\n",
    "print(tfidf_vectorizer.get_feature_names())\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Improvement ideas:\n",
    "    \n",
    "* get rid of figures \"000\", \"cinq\"\n",
    "* get rid of non topic words (destructive approach)\n",
    "* custom build vocabulary for the corpus, based on previous NER extraction \n",
    "* use lemmas (root words) instead of full words (ex: français, france go back to the same entity)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Tf-idf with lemmatized words__\n",
    "\n",
    "Same idea as preprocessing for LDA ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Tf-idf with entity-based vocabulary__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "PER_df=pd.read_csv('articles_PER.csv')\n",
    "LOC_df=pd.read_csv('articles_LOC.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>PER_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>287.000000</td>\n",
       "      <td>287.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>143.000000</td>\n",
       "      <td>5.487805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>82.993976</td>\n",
       "      <td>4.539534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>71.500000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>143.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>214.500000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>286.000000</td>\n",
       "      <td>35.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0   PER_count\n",
       "count  287.000000  287.000000\n",
       "mean   143.000000    5.487805\n",
       "std     82.993976    4.539534\n",
       "min      0.000000    3.000000\n",
       "25%     71.500000    3.000000\n",
       "50%    143.000000    4.000000\n",
       "75%    214.500000    6.000000\n",
       "max    286.000000   35.000000"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PER_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['françois mitterrand', 'jacques chirac', 'ii', 'de gaulle', 'président de la république', 'lionel jospin', 'général de gaulle', 'chirac', 'yasser arafat', 'mitterrand', 'menahem begin', 'bill clinton', 'charles pasqua', 'moshe dayan', 'arafat', 'hier', 'staline', 'alain juppé', 'edouard balladur', '-major', 'washington', 'begin', '-aviv', '-ci', 'itzhak rabin', 'sadate', 'lénine', 'golda meir', 'fini', 'helmut kohl', 'pierre bérégovoy', 'martine aubry', 'kennedy', 'hitler', \"valéry giscard d'estaing\", 'nasser', 'rabin', 'raymond barre', 'expert', 'shimon peres', 'napoléon', 'eisenhower', 'hasard', 'giscard', 'george bush', 'mao', 'saddam hussein', 'boulin', 'delmas', 'david ben gourion', 'robert badinter', 'georges pompidou', 'anouar el-sadate', 'voulez', 'jean-paul ii', 'christ', 'jimmy carter', 'michel rocard', 'voltaire', 'pierre mendès france', 'pierre mauroy', 'robert', 'regardez', 'marx', 'françoise giroud', 'soudain', 'mendès france', 'pompidou', 'louis xiv', 'mikhaïl gorbatchev', 'churchill', 'deng xiaoping', 'bernard kouchner', 'boris eltsine', 'roosevelt', 'claude bébéar', 'jacques toubon', 'guy mollet', '-', 'alain madelin', 'philippe séguin', 'fidel castro', 'john kennedy', 'françois hollande']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>PER_entity</th>\n",
       "      <th>PER_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>françois mitterrand</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>jacques chirac</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>ii</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>de gaulle</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>président de la république</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                  PER_entity  PER_count\n",
       "0           0         françois mitterrand         35\n",
       "1           1              jacques chirac         31\n",
       "2           2                          ii         29\n",
       "3           3                   de gaulle         26\n",
       "4           4  président de la république         26"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PER_highest=PER_df[PER_df['PER_count']>5]\n",
    "print(list(PER_highest.PER_entity))\n",
    "PER_highest.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>LOC_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>439.00000</td>\n",
       "      <td>439.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>219.00000</td>\n",
       "      <td>9.621868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>126.87264</td>\n",
       "      <td>17.268576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>109.50000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>219.00000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>328.50000</td>\n",
       "      <td>8.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>438.00000</td>\n",
       "      <td>158.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0   LOC_count\n",
       "count   439.00000  439.000000\n",
       "mean    219.00000    9.621868\n",
       "std     126.87264   17.268576\n",
       "min       0.00000    3.000000\n",
       "25%     109.50000    3.000000\n",
       "50%     219.00000    5.000000\n",
       "75%     328.50000    8.500000\n",
       "max     438.00000  158.000000"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LOC_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['paris', 'france', 'la france', 'etat', 'etats-unis', 'europe', '-', \"l'europe\", 'allemagne', 'américains', 'état', 'de france', 'israël', 'londres', 'jérusalem', 'français', 'grande-bretagne', 'moscou', 'terre', 'washington', 'chine', 'afrique', '»', 'la chine', 'amérique', 'new york', 'algérie', 'allemands', 'angleterre', 'occident', 'berlin', 'italie', 'japon', 'beyrouth', 'espagne', 'genève', 'russie', 'suisse', 'pékin', \"l'amérique\", 'bruxelles', 'soleil', \"l'italie\", 'f', 'gaza', 'liban', 'cisjordanie', 'syrie', 'la terre', 'jordanie', 'iran', 'sinaï', 'asie', 'mercredi', 'damas', 'cher', 'lyon', 'européens', 'lune', \"l'afrique\", 'russes', 'pologne', 'egypte', 'algériens', 'israéliens', 'rome', 'c?ur', 'anglais', 'alger', 'inde', 'cuba', 'arabie saoudite', 'matignon', 'marseille', 'irak', 'vienne', \"l'empire\", 'madrid', 'pacifique', 'hongrie', 'suède', 'bordeaux', \"l'algérie\", 'tunisie', 'strasbourg', 'atlantique', 'britanniques', 'soudan', \"quai d'orsay\", 'mexique', 'belgique', 'sienne', 'maroc', 'chinois', 'japonais', 'italiens', 'téhéran', 'los angeles', \"l'asie\", 'champagne', 'syriens', 'turquie', 'munich', 'egyptiens', 'afrique du sud', 'libye', 'avais', 'danemark', 'américain', 'hexagone', 'portugal', 'au caire', 'koweït', 'méditerranée', 'le soleil', 'canal de suez', 'vatican', 'vichy', 'kremlin', \"l'espagne\", 'caire', 'roumanie', 'tchécoslovaquie', 'norvège', 'tokyo', 'le japon', 'autriche', 'canada', 'versailles', \"côte-d'ivoire\", 'kenya', 'maghreb', 'pakistan', 'laos', 'sénégal', 'francfort', 'vietnam', 'loire', 'toulouse', 'californie', 'prague', 'grèce', 'nigeria', 'gatt', 'du japon', 'madagascar', 'corée', 'congo', 'gabon', 'croyez', 'australie', 'voudrais', 'cambodge', 'luxembourg']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>LOC_entity</th>\n",
       "      <th>LOC_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>paris</td>\n",
       "      <td>158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>france</td>\n",
       "      <td>156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>la france</td>\n",
       "      <td>144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>etat</td>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>etats-unis</td>\n",
       "      <td>116</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  LOC_entity  LOC_count\n",
       "0           0       paris        158\n",
       "1           1      france        156\n",
       "2           2   la france        144\n",
       "3           3        etat        124\n",
       "4           4  etats-unis        116"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LOC_highest=LOC_df[LOC_df['LOC_count']>6]\n",
    "print(list(LOC_highest.LOC_entity))\n",
    "LOC_highest.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary=['paris', 'france', 'etat', 'etats-unis', 'europe', 'allemagne', 'américains', 'état', 'de france', 'israël', 'londres', 'jérusalem', 'français', 'grande-bretagne', 'moscou', 'terre', 'washington', 'chine', 'afrique', '»', 'la chine', 'amérique', 'new york', 'algérie', 'allemands', 'angleterre', 'occident', 'berlin', 'italie', 'japon', 'beyrouth', 'espagne', 'genève', 'russie', 'suisse', 'pékin', \"l'amérique\", 'bruxelles', 'soleil', \"l'italie\", 'f', 'gaza', 'liban', 'cisjordanie', 'syrie', 'la terre', 'jordanie', 'iran', 'sinaï', 'asie', 'mercredi', 'damas', 'cher', 'lyon', 'européens', 'lune', \"l'afrique\", 'russes', 'pologne', 'egypte', 'algériens', 'israéliens', 'rome', 'c?ur', 'anglais', 'alger', 'inde', 'cuba', 'arabie saoudite', 'matignon', 'marseille', 'irak', 'vienne', \"l'empire\", 'madrid', 'pacifique', 'hongrie', 'suède', 'bordeaux', \"l'algérie\", 'tunisie', 'strasbourg', 'atlantique', 'britanniques', 'soudan', \"quai d'orsay\", 'mexique', 'belgique', 'sienne', 'maroc', 'chinois', 'japonais', 'italiens', 'téhéran', 'los angeles', \"l'asie\", 'champagne', 'syriens', 'turquie', 'munich', 'egyptiens', 'afrique du sud', 'libye', 'avais', 'danemark', 'américain', 'hexagone', 'portugal', 'au caire', 'koweït', 'méditerranée', 'le soleil', 'canal de suez', 'vatican', 'vichy', 'kremlin', \"l'espagne\", 'caire', 'roumanie', 'tchécoslovaquie', 'norvège', 'tokyo', 'le japon', 'autriche', 'canada', 'versailles', \"côte-d'ivoire\", 'kenya', 'maghreb', 'pakistan', 'laos', 'sénégal', 'francfort', 'vietnam', 'loire', 'toulouse', 'californie', 'prague', 'grèce', 'nigeria', 'gatt', 'du japon', 'madagascar', 'corée', 'congo', 'gabon', 'croyez', 'australie', 'voudrais', 'cambodge', 'luxembourg']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
